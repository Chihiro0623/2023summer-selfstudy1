# 연구노트 2주차 (7/10 - 7/14)
## 활동 내용
Classification 논문 Review  
Classification 모델 구현

## 논문 Review
| Week   | Paper                                               | Conf | Year   | Review   |
| :----: | ------------------------------------------------------- | :----: | :------------: | :------: |
| 2    | [Squeeze-and-Excitation Networks](https://arxiv.org/pdf/1709.01507.pdf)<br>[CBAM: Convolutional Block Attention Module](https://arxiv.org/pdf/1807.06521.pdf)<br>[EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/pdf/1905.11946.pdf)<br>[Distilling the Knowledge in a Neural Network](https://arxiv.org/pdf/1503.02531.pdf) | CVPR<br>ECCV<br>ICML<br>NIPs    |2018<br>2018<br>2019<br>2014 | [Review](https://github.com/Chihiro0623/2023summer-selfstudy1/blob/main/week2/Reviews/Squeeze-and-Excitation%20Networks.pdf)<br>[Review](https://github.com/Chihiro0623/2023summer-selfstudy1/blob/main/week2/Reviews/CBAM%20Convolutional%20Block%20Attention%20Module.pdf)<br>[Review]()<br>[Review]() |



### Squeeze-and-Excitation Networks
Channel-wise Training의 중요성을 알려준 논문

### CBAM: Convolutional Block Attention Module
Attention 기법을 이용하여 Channel-wise뿐만 아니라 Spatial도 고려함

### EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks


### Distilling the Knowledge in a Neural Network




## 구현
[Assignment 1](https://github.com/Chihiro0623/2023summer-selfstudy1/blob/main/week1/Project/week1.pdf)


[보고서](https://github.com/Chihiro0623/2023summer-selfstudy1/blob/main/week2/Project/Assignment1.pdf)
